#Deeplearning #completed #nueralNetwork 
## Topic üìñ: Gradient Descent

## Key Concepts üîë:

1. The value given by [[Cost Function]] will help us to learn whether the model needs further **tweaks** or not.

2. If we decide to *improve* the model, the method using which we do this is known as **gradient descent**.

3. Improving the model involves finding the ***appropriate*** weights for the model this can be done by seeing which direction in the multidimensional space should the input move.

4.  This can be done by finding **gradient**, it gives the direction for maximum increase we will take the **negative** for going opposite with small steps.

5. The answer for this gradient descent will give values with either positive or negative direction and magnitude and this magnitude can be interpreted as the importance of that particular weight to that cost function. 

## My Insights üî≠:

- The way we can see the cost's function can be as a simple function which take a single input and gives out a single output.

- In this case, we have to find the **input** (weights) which gives out a minimum **output** (loss).

- So, after all this time I finally learned that cost function is a heart piece of a network and not something to mess with.

- The main takeaway should be that the values that this gradient descent gives out can be interpreted as ‚ÄúThe importance of that weight in reducing the cost function or say improving the performance of the model.‚Äù

## Links/References üîó:
- [[https://youtu.be/IHZwWFHWa-w?si=UExyjRP3pUHRmgMU|3blue1brown's video]]
