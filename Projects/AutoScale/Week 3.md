# ğŸŸ§ WEEK 3 â€” Deep Forecasting (Informer-lite)

**Theme:** Stop reading. Start building.  
**Outcome:** A working LSTM-based workload predictor that forecasts the next 12 steps.

---

## ğŸŸ§ Day 1 â€” Data Pipeline & Framing the Problem

### ğŸ¯ Goal:
Turn raw metrics into a supervised learning problem.

### âœ… Tasks:
- [x] Choose **one metric** as workload signal (CPU or RPS)
- [x] Load historical data (from Week 2 scrape)
- [ ] Normalize the series (MinMax / StandardScaler)
- [ ] Decide:
  - [ ] input window size (e.g. 24 or 60)
  - [ ] prediction horizon = 12
- [ ] Implement sliding window dataset:
  - X â†’ past `window_size`
  - y â†’ next `12` values
- [ ] Plot:
  - raw series
  - one sample window â†’ target

ğŸ“Œ **Checkpoint:**  
You can clearly explain:  
â€œGiven the past ___ points, I predict the next 12.â€

---

## ğŸŸ§ Day 2 â€” LSTM Model (Single-Step First)

### ğŸ¯ Goal:
Get a **minimal LSTM** to learn *anything*.

### âœ… Tasks:
- [ ] Implement basic LSTM model:
  - input â†’ LSTM â†’ Linear
- [ ] Start with **single-step prediction**
- [ ] Choose loss:
  - [ ] MSE
- [ ] Train for a few epochs
- [ ] Plot:
  - predicted vs actual (1-step)

ğŸ“Œ **Rule:**  
No fancy tricks. If this doesnâ€™t learn, stop and debug.

ğŸ“Œ **Checkpoint:**  
Loss goes down. Predictions are not random noise.

---

## ğŸŸ§ Day 3 â€” Multi-Step Forecasting (Next 12 Steps)

### ğŸ¯ Goal:
Predict **12 future steps at once**.

### âœ… Tasks:
- [ ] Modify model output â†’ size 12
- [ ] Update dataset target accordingly
- [ ] Train multi-step LSTM
- [ ] Plot:
  - true future vs predicted future (12-step curve)
- [ ] Measure:
  - [ ] MSE per horizon
  - [ ] overall MSE

ğŸ“Œ **Checkpoint:**  
You can visually see trend continuation (even if imperfect).

---

## ğŸŸ§ Day 4 â€” GRU Variant (Comparison Day)

### ğŸ¯ Goal:
Understand *why* GRU sometimes works better.

### âœ… Tasks:
- [ ] Replace LSTM with GRU
- [ ] Keep everything else identical
- [ ] Train GRU model
- [ ] Compare:
  - [ ] convergence speed
  - [ ] stability
  - [ ] prediction smoothness
- [ ] Write short notes:
  - when GRU felt better
  - when LSTM felt better

ğŸ“Œ **Checkpoint:**  
You understand this is a **design choice**, not dogma.

---

## ğŸŸ§ Day 5 â€” Transformer Encoder (Minimal, Not Fancy)

### ğŸ¯ Goal:
Touch transformers without drowning.

### âœ… Tasks:
- [ ] Implement:
  - positional encoding
  - transformer encoder block
- [ ] Input shape: (sequence_length, features)
- [ ] Output â†’ next 12 steps
- [ ] Train on same dataset
- [ ] Plot predictions

ğŸ“Œ **Rule:**  
No Informer. No attention hacks.  
Just encoder â†’ linear head.

ğŸ“Œ **Checkpoint:**  
You *understand* what attention is doing, even if performance is similar.

---

## ğŸŸ§ Day 6 â€” Evaluation & Failure Analysis

### ğŸ¯ Goal:
Stop celebrating. Start judging.

### âœ… Tasks:
- [ ] Compare:
  - AR (Week 1)
  - LSTM
  - GRU
  - Transformer
- [ ] Metrics:
  - [ ] MSE
  - [ ] MAE
- [ ] Test on:
  - smooth workload
  - bursty workload
- [ ] Identify:
  - where each model fails
  - lag vs overshoot behavior

ğŸ“Œ **Checkpoint:**  
You can justify *why* one model is chosen for Autoscale.

---

## ğŸŸ§ Day 7 â€” Freeze the Predictor (Engineering Day)

### ğŸ¯ Goal:
Prepare this for integration later.

### âœ… Tasks:
- [ ] Choose **one final predictor**
- [ ] Wrap it as a clean Python module:
  - `fit()`
  - `predict_next_12()`
- [ ] Save / load model weights
- [ ] Add inference-only script
- [ ] Write a short `week3_notes.md`:
  - design choices
  - lessons learned
  - mistakes made

ğŸ“Œ **End-of-Week Win:**  
You now have a **production-shaped workload forecaster**.

---

## ğŸ‰ WEEK 3 COMPLETE

You are officially past:
- toy ML
- notebook-only experiments

Next week = **burst detection**, where prediction meets reality.
