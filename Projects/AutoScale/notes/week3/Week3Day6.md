# ðŸŸ§ Day 6 â€” Evaluation & Failure Analysis

## ðŸŽ¯ Objective

Evaluate all models **as autoscaling decision systems**, not as ML benchmarks.

The goal is to understand **how each model fails**, and which failure modes are acceptable in a production autoscaling environment.

---

## ðŸ§ª Models Evaluated

- **AR (Week 1)** â€” linear autoregressive baseline
    
- **LSTM** â€” recurrent model with explicit long-term memory
    
- **GRU** â€” simplified recurrent model with faster, smoother memory
    
- **Transformer Encoder** â€” attention-based temporal mixing model
    

All models were trained and evaluated on the **same dataset**, using the **same prediction horizon (12 steps)**.

---

## ðŸ“ Evaluation Metrics

### Mean Squared Error (MSE)

- Penalizes large errors heavily
    
- Sensitive to spikes and extreme misses
    
- Proxy for **risk** (under-provisioning is costly)
    

### Mean Absolute Error (MAE)

- Measures average deviation
    
- Less sensitive to outliers
    
- Proxy for **cost efficiency**
    

**Autoscale interpretation:**

- MSE â‰ˆ safety & SLA risk
    
- MAE â‰ˆ resource efficiency
    

Both metrics are necessary.

---

## ðŸŒŠ Workload Types Tested

### ðŸŸ¢ Smooth Workload

Characteristics:

- Gradual trends
    
- Low variance
    
- Predictable behavior
    

Used to test:

- stability
    
- overfitting
    
- unnecessary model complexity
    

---

### ðŸ”´ Bursty Workload

Characteristics:

- Sudden spikes
    
- Sharp drops
    
- Non-stationary patterns
    

Used to test:

- lag
    
- overshoot
    
- failure severity
    

This workload is **critical** for autoscaling.

---

## ðŸ”¬ Model-by-Model Failure Analysis

### ðŸ§® AR (Autoregressive Baseline)

**Failure Mode**

- Strong lag during sudden spikes
    
- No ability to adapt to regime changes
    
- Blind linear extrapolation
    

**Observed Behavior**

- Consistently under-predicts bursts
    
- Predictable but slow response
    
- Low variance, low adaptability
    

**Autoscale Verdict**

- âŒ Unsafe as a standalone predictor
    
- âœ… Useful as a baseline and sanity check
    

---

### ðŸ§  LSTM

**Failure Mode**

- Overreacts to recent patterns
    
- Retains outdated memory after regime shifts
    
- Sensitive to hyperparameters
    

**Observed Behavior**

- Captures long-term structure well
    
- May overshoot after spikes
    
- Slower convergence compared to GRU
    

**Autoscale Verdict**

- âš ï¸ Powerful but risky
    
- Requires careful tuning and safeguards
    
- Overshoot risk acceptable, memory inertia is not
    

---

### âš¡ GRU

**Failure Mode**

- Smooths sharp spikes
    
- Underestimates rare extreme events
    
- Faster forgetting of anomalies
    

**Observed Behavior**

- Very stable training
    
- Fast convergence
    
- Conservative, mean-oriented predictions
    

**Autoscale Verdict**

- âœ… Best default choice
    
- Predictable failures
    
- Safe when combined with buffer margins
    

---

### ðŸ§ âš¡ Transformer Encoder (Minimal)

**Failure Mode**

- Bottleneck collapse into single vector
    
- Limited spike preservation
    
- Complexity not fully utilized
    

**Observed Behavior**

- Slightly better mid-range alignment than GRU
    
- Less aggressive smoothing
    
- Performance gains marginal under current architecture
    

**Autoscale Verdict**

- ðŸ§ª Promising but unjustified in current form
    
- Requires decoder or relaxed bottleneck to shine
    
- Overkill for current Autoscale stage
    

---

## â±ï¸ Lag vs Overshoot (Critical Autoscale Lens)

### Lag

- Causes under-provisioning
    
- Leads to outages and SLA violations
    
- **Unacceptable**
    

### Overshoot

- Wastes resources
    
- Increases cost
    
- **Acceptable and manageable**
    

**Preferred failure pattern:**

> Predictable overshoot > unpredictable lag

---

## ðŸ§  Comparative Summary

|Model|Main Failure|Risk Profile|Autoscale Suitability|
|---|---|---|---|
|AR|Lag|High|âŒ|
|LSTM|Overshoot + memory inertia|Medium|âš ï¸|
|GRU|Smoothing spikes|Low|âœ…|
|Transformer|Bottleneck-limited|Medium|ðŸ§ª|

---

## âœ… Final Justification (Checkpoint)

**GRU is chosen for Autoscale** because:

- It converges quickly
    
- It is stable and predictable
    
- Its failure mode (smoothing) is safer than lag
    
- Safety margins can compensate for underestimation
    

Other models either:

- react too slowly (AR),
    
- react too aggressively (LSTM),
    
- or require architectural expansion (Transformer).
    

---

## ðŸ Key Takeaway

Model choice for autoscaling is **not about accuracy alone**.

> It is about **how the model fails**,  
> and whether those failures are survivable in production.

Day 6 marks the shift from **model building** to **system design**.